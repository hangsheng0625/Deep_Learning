{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hangsheng0625/Deep_Learning/blob/main/Week11_Kaggle_ViT_ModelFineTuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7YV68y4YRel"
      },
      "source": [
        "# <span style=\"color:#0b486b\">  FIT3181/5215: In-class Kaggle Competition</span>\n",
        "***\n",
        "*CE/Lecturer (Clayton):*  **Dr Trung Le** | trunglm@monash.edu <br/>\n",
        "*Lecturer (Clayton):* **Prof Dinh Phung** | dinh.phung@monash.edu <br/>\n",
        "*Lecturer (Malaysia):*  **Dr Arghya Pal** | arghya.pal@monash.edu <br/> <br/>\n",
        " <br/>\n",
        "Faculty of Information Technology, Monash University, Australia\n",
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <span style=\"color:#0b486b\"> Kaggle week 11: ViT Transfer learning, Model Fine-Tuning with prompts and Adapters\n",
        "\n",
        "**Your roles:**\n",
        "- Leveraging ViT fine-tuning for classification tasks.\n",
        "- You can adopt either ViT transfer learning, model fine-tuning with prompts, or fine-tuning with adapters.\n",
        "- The dataset consists of 5640 images across 47 categories, with 1880 images each for training, validation, and testing.\n",
        "- Predict the test set of 1880 images and submit your solution to Kaggle."
      ],
      "metadata": {
        "id": "anDmcnrDA6C4"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3IPXkpb5cYPI"
      },
      "source": [
        "# Setup\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown https://drive.google.com/file/d/1BTSDJiLTG6wlrTtIGOhy8sZl73xJSynf/view?usp=sharing --fuzzy # comment out for the second run\n",
        "# backup\n",
        "# https://drive.google.com/file/d/1srQLZo461jmYJhtG-P-gxNxEH7LbxaES/view?usp=sharing\n",
        "# https://drive.google.com/file/d/1NKSuoOTxiT4LQA4Oa4PRbrt0tBmsq6aC/view?usp=sharing\n",
        "# https://drive.google.com/file/d/1tDF8dN-rZyQ2_urp2WSAMfffgrfeht9-/view?usp=sharing"
      ],
      "metadata": {
        "id": "xMi4kcX7Vk3L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88ca9baf-ea8b-4c33-a596-c7f77b92da48"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1BTSDJiLTG6wlrTtIGOhy8sZl73xJSynf\n",
            "From (redirected): https://drive.google.com/uc?id=1BTSDJiLTG6wlrTtIGOhy8sZl73xJSynf&confirm=t&uuid=4beb237d-c09f-479b-9c84-6261f645a893\n",
            "To: /content/Kaggle_Week10.zip\n",
            "100% 626M/626M [00:09<00:00, 66.0MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -q -o Kaggle_Week10.zip # comment out for the second run"
      ],
      "metadata": {
        "id": "WGzqL6CpVlyK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQwb0c1Getlj"
      },
      "source": [
        "Download the pre-trained ViT-B_16 and store on Google Colab drive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jozrGU_3SXab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e301d3bf-777d-45ab-df72-68c812e9b169"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-09-29 11:01:36--  https://storage.googleapis.com/vit_models/imagenet21k/ViT-B_16.npz\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.130.207, 74.125.68.207, 64.233.170.207, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.130.207|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 412815506 (394M) [application/octet-stream]\n",
            "Saving to: ‘ViT-B_16.npz.2’\n",
            "\n",
            "ViT-B_16.npz.2      100%[===================>] 393.69M  12.4MB/s    in 35s     \n",
            "\n",
            "2024-09-29 11:02:13 (11.3 MB/s) - ‘ViT-B_16.npz.2’ saved [412815506/412815506]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Load imagenet21k pre-train weights for ViT-B_16\n",
        "!wget https://storage.googleapis.com/vit_models/imagenet21k/ViT-B_16.npz # comment out for the second run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IrIKwppunC1w",
        "outputId": "12edb46b-6008-499d-cffc-b08e492369ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ml_collections in /usr/local/lib/python3.10/dist-packages (0.1.1)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from ml_collections) (1.4.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from ml_collections) (6.0.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from ml_collections) (1.16.0)\n",
            "Requirement already satisfied: contextlib2 in /usr/local/lib/python3.10/dist-packages (from ml_collections) (21.6.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install ml_collections"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-lN4EjuC3gMB"
      },
      "source": [
        "We import the necessary libraries and packages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zuEnZwUInADb"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.simplefilter(\"ignore\", UserWarning)\n",
        "\n",
        "import copy\n",
        "import logging\n",
        "import math\n",
        "from tqdm import tqdm\n",
        "import ml_collections\n",
        "\n",
        "from os.path import join as pjoin\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "\n",
        "from torch.nn import CrossEntropyLoss, Dropout, Softmax, Linear, Conv2d, LayerNorm\n",
        "from torch.nn.modules.utils import _pair\n",
        "from scipy import ndimage\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D3TzcqMLVNXF"
      },
      "outputs": [],
      "source": [
        "from Kaggle_Week10.vit import Embeddings, Mlp, Attention, Transformer, VisionTransformer\n",
        "from Kaggle_Week10.training import predict_and_save, load_test_images #, train_epoch_vit"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gsJ4lVXi3uN7"
      },
      "source": [
        "# Prepare the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "88yL97K5fA56",
        "outputId": "69cc1c58-b0c7-4980-a4d3-d9b256e54294",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 1000 images\n",
            "Number of classes: 47\n",
            "Number of training samples: 1880\n",
            "Number of validation samples: 1880\n",
            "Number of test samples: 1880\n",
            "Batch shape: torch.Size([32, 3, 224, 224])\n",
            "Labels shape: torch.Size([32])\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "data_dir = './Kaggle_Week10/dataset'\n",
        "\n",
        "batch_size = 32\n",
        "img_size = 224\n",
        "# Data augmentation for training set\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(img_size),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "# test transform\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.Resize((img_size, img_size)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "# Load datasets\n",
        "train_dataset = datasets.ImageFolder(root=os.path.join(data_dir, 'train'), transform=train_transform)\n",
        "val_dataset = datasets.ImageFolder(root=os.path.join(data_dir, 'val'), transform=test_transform)\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
        "test_loader = load_test_images(os.path.join(data_dir, 'test'))\n",
        "\n",
        "class_names = train_dataset.classes\n",
        "\n",
        "# Usage\n",
        "\n",
        "print(f\"Number of classes: {len(class_names)}\")\n",
        "# Print some information about the datasets\n",
        "print(f\"Number of training samples: {len(train_loader.dataset)}\")\n",
        "print(f\"Number of validation samples: {len(val_loader.dataset)}\")\n",
        "print(f\"Number of test samples: {len(test_loader.dataset)}\")\n",
        "# Fetch a batch of training data\n",
        "images, labels = next(iter(train_loader))\n",
        "print(f\"Batch shape: {images.shape}\")\n",
        "print(f\"Labels shape: {labels.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dkhYVNpeVNXG"
      },
      "source": [
        "# Prepare the backbone"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CnQ6Yb3z5lMj"
      },
      "source": [
        "This code snippet outlines a set of default configurations and utility functions for the Vision Transformer (ViT) model. ViT is a Transformer-based model designed for image classification, where images are processed as a sequence of patches instead of individual pixels, leveraging the attention mechanism traditionally used in NLP. The configuration defines key model components such as the multi-head attention query (`ATTENTION_Q`), key (`ATTENTION_K`), value (`ATTENTION_V`), and output (`ATTENTION_OUT`) layers, as well as fully connected layers (`FC_0` and `FC_1`) within the multi-layer perceptron (MLP) block and normalization layers (`ATTENTION_NORM` and `MLP_NORM`).\n",
        "\n",
        "The utility function `np2th` is used to convert weights from a NumPy format into a PyTorch tensor, with an option to adjust tensor shapes for convolutional layers. Additionally, the code includes an activation function dictionary (`ACT2FN`), which maps activation function names (like `gelu`, `relu`, and `swish`) to their respective implementations in PyTorch. This setup facilitates flexibility in selecting the desired activation functions and ensures smooth conversion of weights for initializing the ViT model in a PyTorch environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tkojb1aLY0b-"
      },
      "outputs": [],
      "source": [
        "# VIT default config\n",
        "ATTENTION_Q = \"MultiHeadDotProductAttention_1/query\"\n",
        "ATTENTION_K = \"MultiHeadDotProductAttention_1/key\"\n",
        "ATTENTION_V = \"MultiHeadDotProductAttention_1/value\"\n",
        "ATTENTION_OUT = \"MultiHeadDotProductAttention_1/out\"\n",
        "FC_0 = \"MlpBlock_3/Dense_0\"\n",
        "FC_1 = \"MlpBlock_3/Dense_1\"\n",
        "ATTENTION_NORM = \"LayerNorm_0\"\n",
        "MLP_NORM = \"LayerNorm_2\"\n",
        "\n",
        "def np2th(weights, conv=False):\n",
        "    \"\"\"Possibly convert HWIO to OIHW.\"\"\"\n",
        "    if conv:\n",
        "        weights = weights.transpose([3, 2, 0, 1])\n",
        "    return torch.from_numpy(weights)\n",
        "\n",
        "def swish(x):\n",
        "    return x * torch.sigmoid(x)\n",
        "\n",
        "\n",
        "ACT2FN = {\"gelu\": torch.nn.functional.gelu, \"relu\": torch.nn.functional.relu, \"swish\": swish}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bXrPgQLy7Zy0"
      },
      "source": [
        "We create a ViT_B_16 model and then load the weights of the pre-trained ViT_B_16 model to our declared ViT_B_16 architecture.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wXz8pHSflb1w"
      },
      "source": [
        "The `get_b16_config` function is designed to return the configuration settings for the Vision Transformer (ViT) model with a base architecture (ViT-B) using 16x16 pixel patches. It specifies various parameters such as hidden size, number of transformer layers, attention heads, and dropout rates, which are crucial for initializing the model's architecture and training it effectively. This configuration serves as a foundational setup for the ViT model, ensuring that it operates as intended for image classification tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DahsgX1vlrFX"
      },
      "outputs": [],
      "source": [
        "def get_b16_config():\n",
        "    \"\"\"Returns the ViT-B/16 configuration.\"\"\"\n",
        "    config = ml_collections.ConfigDict()\n",
        "    config.patches = ml_collections.ConfigDict({'size': (16, 16)})  # Set patch size to 16x16\n",
        "    config.hidden_size = 768  # Define hidden size for the transformer\n",
        "    config.transformer = ml_collections.ConfigDict()\n",
        "    config.transformer.mlp_dim = 3072  # Set the dimension of the MLP in the transformer\n",
        "    config.transformer.num_heads = 12  # Define the number of attention heads\n",
        "    config.transformer.num_layers = 12  # Set the number of transformer layers\n",
        "    config.transformer.attention_dropout_rate = 0.0  # Set dropout rate for attention\n",
        "    config.transformer.dropout_rate = 0.1  # Set general dropout rate\n",
        "    config.classifier = 'token'  # Specify classifier type\n",
        "    config.representation_size = None  # Set representation size (None for default)\n",
        "    return config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eks-WgvLW317"
      },
      "outputs": [],
      "source": [
        "config = get_b16_config()\n",
        "pretrained_dir = \"./ViT-B_16.npz\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HvASjFsDVNXH"
      },
      "source": [
        "# Finetuning ViT models"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_epoch_vit(model, optimizer, train_loader, test_loader, scheduler, e):\n",
        "  running_loss = 0.\n",
        "  running_acc = 0\n",
        "  num_data = 0\n",
        "  model.train()\n",
        "  lr = scheduler.get_lr()[0]\n",
        "  loss_fct = CrossEntropyLoss(reduction='mean')\n",
        "  with tqdm(total=len(train_loader), desc=\"Epoch {}\".format(e)) as tepoch:\n",
        "    for i, data in enumerate(train_loader):\n",
        "        tepoch.update(1)\n",
        "        # Every data instance is an input + label pair\n",
        "        inputs, labels = [d_i.cuda() for d_i in data]\n",
        "        # Zero your gradients for every batch!\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Make predictions for this batch\n",
        "        outputs = model(inputs)\n",
        "        loss = loss_fct(outputs, labels)\n",
        "        acc = 100.*(torch.argmax(outputs, 1) == labels).sum()\n",
        "        loss.backward()\n",
        "\n",
        "        # Adjust learning weights\n",
        "        optimizer.step()\n",
        "\n",
        "        # Gather data and report\n",
        "        running_loss += len(labels)*loss.cpu().item()\n",
        "        running_acc += acc.cpu().item()\n",
        "        num_data += len(labels)\n",
        "        tepoch.set_postfix({\"Loss\": \"={:.3f}, Acc={:.2f}, Lr={:.4f}\".format(running_loss/num_data, running_acc/num_data, lr)})\n",
        "    tepoch.close()\n",
        "  scheduler.step()\n",
        "  if e%2==0: # you can modify the period to validate the model for speed up training\n",
        "    running_loss = 0.\n",
        "    running_acc = 0\n",
        "    num_data = 0\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "      with tqdm(total=len(test_loader), desc=\"\\tTesting\") as tepoch:\n",
        "        for i, data in enumerate(test_loader):\n",
        "          tepoch.update(1)\n",
        "          inputs, labels = [d_i.cuda() for d_i in data]\n",
        "          outputs = model(inputs)\n",
        "          loss = loss_fct(outputs, labels)\n",
        "          acc = 100.*(torch.argmax(outputs, 1) == labels).sum()\n",
        "          # Gather data and report\n",
        "          running_loss += len(labels)*loss.item()\n",
        "          running_acc += acc.item()\n",
        "          num_data += len(labels)\n",
        "          tepoch.set_postfix({\"Loss\": \"={:.3f}, Acc={:.2f}\".format(running_loss/num_data, running_acc/num_data)})\n",
        "      tepoch.close()"
      ],
      "metadata": {
        "id": "rjneL1yDbZ7W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DpeBGfk3VNXH"
      },
      "source": [
        "# I) Transfer learning and Finetuning ViT models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DZXAt0nHmcwE"
      },
      "source": [
        "A Vision Transformer model is initialized with a specified configuration and image size, and pre-trained weights are loaded from a given directory. The model is then moved to the GPU for computation. An SGD optimizer is created with a learning rate of 0.01 and a momentum of 0.9 to optimize the model's parameters during training. Additionally, a cosine annealing learning rate scheduler is set up to adjust the learning rate over 50 epochs, allowing it to decrease gradually, which can help improve training stability and performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O0XJ-_2fhzT1"
      },
      "outputs": [],
      "source": [
        "epochs = 10\n",
        "num_classes = 47\n",
        "model = VisionTransformer(config, img_size, zero_head=True, num_classes=num_classes)\n",
        "model.load_from(np.load(pretrained_dir))\n",
        "model.cuda()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZzV4q-ATmo8F"
      },
      "source": [
        "#### Training ViT models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mIKgEi0SXfNb",
        "outputId": "767fddf5-82e2-47c5-d67f-a2098779ff64"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 0: 100%|██████████| 59/59 [01:02<00:00,  1.06s/it, Loss==3.670, Acc=37.87, Lr=0.0100]\n",
            "\tTesting: 100%|██████████| 59/59 [00:23<00:00,  2.47it/s, Loss==3.383, Acc=58.40]\n"
          ]
        }
      ],
      "source": [
        "for e in range(epochs):\n",
        "  train_epoch_vit(model, optimizer, train_loader, val_loader, scheduler, e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JSt8w_46VNXI"
      },
      "source": [
        "## Submit to Kaggle for VIT transfer learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z-uFImDKVNXI"
      },
      "outputs": [],
      "source": [
        "## create a csv file for fine-tuning for ViTs, and you can submit solution_normal_vit_finetuning.csv to Kaggle\n",
        "predict_and_save(model, test_loader, device=\"cuda\", submission_path=\"solution_normal_vit_finetuning.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m1Vu3posY1z0"
      },
      "source": [
        "# II) Model Fine-Tuning with Prompts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wBHav0jHngIh"
      },
      "source": [
        "The `PromptedTransformer` class extends the base `Transformer` class to incorporate additional prompted tokens into the input embeddings, enhancing the model's ability to capture context or features from the input images. It initializes with a configuration that specifies how to manage prompted tokens, including their number, dropout rates, and initialization strategies. The class features methods for incorporating these prompts into the input data and for processing them through deep layers of the transformer architecture. The `forward` method determines how to handle the input data, either by integrating the prompted tokens directly or utilizing a more complex deep prompting mechanism. Overall, this class is designed to improve performance in tasks where additional context or representation from prompted tokens can enhance the learning capability of the Vision Transformer model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GFJicTpgnVhX"
      },
      "outputs": [],
      "source": [
        "from functools import reduce\n",
        "from operator import mul\n",
        "class PromptedTransformer(Transformer):\n",
        "    def __init__(self, prompt_config, config, img_size, vis):\n",
        "        # Ensure the prompt configuration is set to prepend, indicating\n",
        "        # that the prompt tokens will be added at the beginning of the input.\n",
        "        assert prompt_config.LOCATION == \"prepend\"\n",
        "        # Check that the prompt tokens are to be initialized randomly.\n",
        "        assert prompt_config.INITIATION == \"random\"\n",
        "        # Ensure that no deep layers are specified for the prompt.\n",
        "        assert prompt_config.NUM_DEEP_LAYERS is None\n",
        "        # Confirm that deep sharing of prompts is not enabled.\n",
        "        assert not prompt_config.DEEP_SHARED\n",
        "        # Initialize the parent Transformer class with the provided configurations.\n",
        "        super(PromptedTransformer, self).__init__(config, img_size, vis)\n",
        "        # Store the provided prompt configuration for later use.\n",
        "        self.prompt_config = prompt_config\n",
        "        # Store the Vision Transformer configuration for layer setups.\n",
        "        self.vit_config = config\n",
        "        # Convert the image size and patch size to a standardized format.\n",
        "        img_size = _pair(img_size)\n",
        "        patch_size = _pair(config.patches[\"size\"])\n",
        "        # Get the number of prompt tokens from the prompt configuration.\n",
        "        num_tokens = self.prompt_config.NUM_TOKENS\n",
        "        self.num_tokens = num_tokens  # Store the number of prompted tokens.\n",
        "        # Initialize a dropout layer for the prompt embeddings.\n",
        "        self.prompt_dropout = Dropout(self.prompt_config.DROPOUT)\n",
        "        # Check if prompt embeddings need to be projected to a different dimensionality.\n",
        "        if self.prompt_config.PROJECT > -1:\n",
        "            # Set the prompt dimension to the specified project size.\n",
        "            prompt_dim = self.prompt_config.PROJECT\n",
        "            # Create a linear layer to project the prompt embeddings to the hidden size.\n",
        "            self.prompt_proj = nn.Linear(prompt_dim, config.hidden_size)\n",
        "            # Initialize the weights of the projection layer using Kaiming normal initialization.\n",
        "            nn.init.kaiming_normal_(self.prompt_proj.weight, a=0, mode='fan_out')\n",
        "        else:\n",
        "            # If no projection is required, set the projection layer to be an identity function.\n",
        "            prompt_dim = config.hidden_size\n",
        "            self.prompt_proj = nn.Identity()\n",
        "        # Initialize prompt embeddings based on the specified initiation method.\n",
        "        if self.prompt_config.INITIATION == \"random\":\n",
        "            # Calculate a value for uniform initialization based on the patch size and prompt dimension.\n",
        "            val = math.sqrt(6. / float(3 * reduce(mul, patch_size, 1) + prompt_dim))  # noqa\n",
        "            # Create a parameter for prompt embeddings, initialized to zeros.\n",
        "            self.prompt_embeddings = nn.Parameter(torch.zeros(1, num_tokens, prompt_dim))\n",
        "            # Use uniform initialization for the prompt embeddings.\n",
        "            nn.init.uniform_(self.prompt_embeddings.data, -val, val)\n",
        "            # If deep prompting is enabled, create additional prompt embeddings for deep layers.\n",
        "            if self.prompt_config.DEEP:  # noqa\n",
        "                total_d_layer = config.transformer[\"num_layers\"] - 1\n",
        "                # Create parameters for deep prompt embeddings initialized to zeros.\n",
        "                self.deep_prompt_embeddings = nn.Parameter(torch.zeros(total_d_layer, num_tokens, prompt_dim))\n",
        "                # Use uniform initialization for the deep prompt embeddings.\n",
        "                nn.init.uniform_(self.deep_prompt_embeddings.data, -val, val)\n",
        "        else:\n",
        "            # Raise an error if an unsupported initiation scheme is provided.\n",
        "            raise ValueError(\"Other initiation scheme is not supported\")\n",
        "\n",
        "    def incorporate_prompt(self, x):\n",
        "        # combine prompt embeddings with image-patch embeddings\n",
        "        B = x.shape[0]\n",
        "        # after CLS token, all before image patches\n",
        "        x = self.embeddings(x)  # (batch_size, 1 + n_patches, hidden_dim)\n",
        "        x = torch.cat((\n",
        "                x[:, :1, :],\n",
        "                self.prompt_dropout(self.prompt_proj(self.prompt_embeddings).expand(B, -1, -1)),\n",
        "                x[:, 1:, :]\n",
        "            ), dim=1)\n",
        "        # (batch_size, cls_token + n_prompt + n_patches, hidden_dim)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def train(self, mode=True):\n",
        "        # Set the training status for the PromptedTransformer class.\n",
        "        # This method controls which modules are set to training mode\n",
        "        # and which are set to evaluation mode based on the `mode` argument.\n",
        "        if mode:\n",
        "            # If the mode is True, set the model to training mode:\n",
        "            # Set the encoder to evaluation mode to freeze its parameters\n",
        "            # and prevent updates during training. This is useful when\n",
        "            # the encoder is pre-trained and only the prompt-related\n",
        "            # modules are being fine-tuned.\n",
        "            self.encoder.eval()\n",
        "\n",
        "            # Similarly, set the embeddings layer to evaluation mode,\n",
        "            # ensuring that the embedding parameters do not get updated\n",
        "            # during training, thus keeping them static.\n",
        "            self.embeddings.eval()\n",
        "\n",
        "            # Set the prompt projection layer to training mode, allowing\n",
        "            # its parameters to be updated during training. This layer\n",
        "            # projects the prompt embeddings to match the hidden size.\n",
        "            self.prompt_proj.train()\n",
        "\n",
        "            # Set the prompt dropout layer to training mode, enabling\n",
        "            # dropout during training to prevent overfitting. Dropout\n",
        "            # will randomly zero some of the prompt embeddings based on\n",
        "            # the specified dropout rate.\n",
        "            self.prompt_dropout.train()\n",
        "        else:\n",
        "            # If the mode is False, set the model to evaluation mode:\n",
        "            # Loop through all child modules of the PromptedTransformer\n",
        "            # and set each one to the specified mode (True for training,\n",
        "            # False for evaluation). This ensures that all parts of the\n",
        "            # model can behave correctly depending on the context (i.e.,\n",
        "            # whether it's being trained or evaluated).\n",
        "            for module in self.children():\n",
        "                module.train(mode)\n",
        "\n",
        "    def forward_deep_prompt(self, embedding_output):\n",
        "        # Initialize a list to store attention weights for visualization.\n",
        "        attn_weights = []\n",
        "\n",
        "        # Initialize hidden_states and weights to None.\n",
        "        # hidden_states will store the output of each transformer layer,\n",
        "        # while weights will capture attention weights.\n",
        "        hidden_states = None\n",
        "        weights = None\n",
        "\n",
        "        # Get the batch size from the input embedding output.\n",
        "        B = embedding_output.shape[0]\n",
        "\n",
        "        # Retrieve the number of transformer layers from the configuration.\n",
        "        num_layers = self.vit_config.transformer[\"num_layers\"]\n",
        "\n",
        "        # Loop through each transformer layer.\n",
        "        for i in range(num_layers):\n",
        "            if i == 0:\n",
        "                # For the first layer, directly pass the embedding output\n",
        "                # through the encoder's first layer, capturing the output\n",
        "                # hidden states and attention weights.\n",
        "                hidden_states, weights = self.encoder.layer[i](embedding_output)\n",
        "            else:\n",
        "                # For subsequent layers, check if deep prompt embeddings are being used.\n",
        "                if i <= self.deep_prompt_embeddings.shape[0]:\n",
        "                    # Apply dropout and projection to the deep prompt embeddings for the current layer.\n",
        "                    deep_prompt_emb = self.prompt_dropout(self.prompt_proj(\n",
        "                        self.deep_prompt_embeddings[i-1]).expand(B, -1, -1))\n",
        "\n",
        "                    # Concatenate the CLS token, the processed deep prompt embeddings,\n",
        "                    # and the remaining hidden states from the previous layer.\n",
        "                    hidden_states = torch.cat((\n",
        "                        hidden_states[:, :1, :],  # CLS token\n",
        "                        deep_prompt_emb,          # Deep prompt embeddings\n",
        "                        hidden_states[:, (1+self.num_tokens):, :]  # Remaining tokens\n",
        "                    ), dim=1)\n",
        "\n",
        "                # Pass the updated hidden states through the current transformer layer,\n",
        "                # capturing the new hidden states and attention weights.\n",
        "                hidden_states, weights = self.encoder.layer[i](hidden_states)\n",
        "\n",
        "            # If visualization is enabled, append the attention weights for the current layer.\n",
        "            if self.encoder.vis:\n",
        "                attn_weights.append(weights)\n",
        "\n",
        "        # Apply layer normalization to the final hidden states before returning.\n",
        "        encoded = self.encoder.encoder_norm(hidden_states)\n",
        "\n",
        "        # Return the encoded representations and the list of attention weights.\n",
        "        return encoded, attn_weights\n",
        "\n",
        "    def forward(self, x):\n",
        "        # This is the default forward pass for the PromptedTransformer.\n",
        "        # First, incorporate prompt embeddings into the input tensor x.\n",
        "        embedding_output = self.incorporate_prompt(x)\n",
        "\n",
        "        # Check if deep prompting is enabled in the configuration.\n",
        "        if self.prompt_config.DEEP:\n",
        "            # If deep prompting is enabled, pass the embedding output\n",
        "            # through the deep prompt forward function, which processes\n",
        "            # the embeddings layer by layer and returns the encoded\n",
        "            # output along with attention weights for visualization.\n",
        "            encoded, attn_weights = self.forward_deep_prompt(embedding_output)\n",
        "        else:\n",
        "            # If deep prompting is not enabled, simply pass the embedding output\n",
        "            # through the encoder, which computes the encoded representation\n",
        "            # and also returns attention weights.\n",
        "            encoded, attn_weights = self.encoder(embedding_output)\n",
        "\n",
        "        # Return the encoded representations and the attention weights.\n",
        "        return encoded, attn_weights\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4UbPUn9vzW2J"
      },
      "source": [
        "The `PromptedVisionTransformer` class extends the functionality of the `VisionTransformer` class by incorporating prompting mechanisms. In its constructor, it initializes the base class with various parameters while asserting that the prompt configuration's pooling type is set to \"original.\" It raises an error if the `prompt_cfg` is `None`. The class instantiates a `PromptedTransformer` object, which manages the incorporation of prompts into the transformer architecture. In the `forward` method, it processes the input `x` through the transformer to obtain attention weights and embeddings. It then extracts the first token (typically the class token) from the transformer output, passes it through a classification head to produce logits, and returns the logits, along with the attention weights if visualization is requested."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wDKchetDY6kr"
      },
      "outputs": [],
      "source": [
        "class PromptedVisionTransformer(VisionTransformer):\n",
        "    def __init__(\n",
        "        self, prompt_cfg, model_type=None,\n",
        "        img_size=224, num_classes=21843, vis=False, vit_cfg=None, zero_head=False\n",
        "    ):\n",
        "        assert prompt_cfg.VIT_POOL_TYPE == \"original\"\n",
        "        super(PromptedVisionTransformer, self).__init__(\n",
        "            vit_cfg, img_size, num_classes, zero_head, vis)\n",
        "        if prompt_cfg is None:\n",
        "            raise ValueError(\"prompt_cfg cannot be None if using PromptedVisionTransformer\")\n",
        "        self.prompt_cfg = prompt_cfg\n",
        "        # vit_cfg = CONFIGS[model_type]\n",
        "        self.transformer = PromptedTransformer(prompt_cfg, vit_cfg, img_size, vis)\n",
        "\n",
        "    def forward(self, x, vis=False):\n",
        "        # Pass the input x through the transformer to get the output and attention weights.\n",
        "        x, attn_weights = self.transformer(x)\n",
        "\n",
        "        # Extract the first token from the transformer output, which is typically used for classification.\n",
        "        x = x[:, 0]\n",
        "\n",
        "        # Pass the extracted token through the classification head to obtain the logits.\n",
        "        logits = self.head(x)\n",
        "\n",
        "        # If visualization is not requested, return only the logits.\n",
        "        if not vis:\n",
        "            return logits\n",
        "\n",
        "        # If visualization is requested, return both the logits and the attention weights for further analysis.\n",
        "        return logits, attn_weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jiOmoWoo1TaI"
      },
      "source": [
        "The `get_prompt_config` function defines and returns a configuration dictionary for the ViT-B/16 model, specifically tailored for prompt tuning. The configuration includes parameters such as the number of prompt tokens, their location (set to \"prepend\"), and initialization methods, which can be random or based on final class embeddings. It also specifies settings for deep prompting, including whether to apply it, the number of deep layers, and sharing of prompt embeddings across layers. Additionally, the configuration outlines options for how the output embeddings are pooled for the classification head, along with dropout rates and the option to save the model state after each epoch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ZfH3WUlblUB"
      },
      "outputs": [],
      "source": [
        "def get_prompt_config():\n",
        "    \"\"\"Returns the ViT-B/16 configuration.\"\"\"\n",
        "    config = ml_collections.ConfigDict()\n",
        "    config.NUM_TOKENS = 5\n",
        "    config.LOCATION = \"prepend\"\n",
        "    # prompt initalizatioin:\n",
        "        # (1) default \"random\"\n",
        "        # (2) \"final-cls\" use aggregated final [cls] embeddings from training dataset\n",
        "        # (3) \"cls-nolastl\": use first 12 cls embeddings (exclude the final output) for deep prompt\n",
        "        # (4) \"cls-nofirstl\": use last 12 cls embeddings (exclude the input to first layer)\n",
        "    config.INITIATION = \"random\"  # \"final-cls\", \"cls-first12\"\n",
        "    config.CLSEMB_FOLDER = \"\"\n",
        "    config.CLSEMB_PATH = \"\"\n",
        "    config.PROJECT = -1  # \"projection mlp hidden dim\"\n",
        "    config.DEEP = False # \"whether do deep prompt or not, only for prepend location\"\n",
        "\n",
        "\n",
        "    config.NUM_DEEP_LAYERS = None  # if set to be an int, then do partial-deep prompt tuning\n",
        "    config.REVERSE_DEEP = False  # if to only update last n layers, not the input layer\n",
        "    config.DEEP_SHARED = False  # if true, all deep layers will be use the same prompt emb\n",
        "    config.FORWARD_DEEP_NOEXPAND = False  # if true, will not expand input sequence for layers without prompt\n",
        "    # how to get the output emb for cls head:\n",
        "        # original: follow the orignial backbone choice,\n",
        "        # img_pool: image patch pool only\n",
        "        # prompt_pool: prompt embd pool only\n",
        "        # imgprompt_pool: pool everything but the cls token\n",
        "    config.VIT_POOL_TYPE = \"original\"\n",
        "    config.DROPOUT = 0.0\n",
        "    config.SAVE_FOR_EACH_EPOCH = False\n",
        "    return config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5CFWCJlScyt_"
      },
      "outputs": [],
      "source": [
        "prompt_config = get_prompt_config()\n",
        "vit_config = get_b16_config()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O-BGwbY_dEHL"
      },
      "outputs": [],
      "source": [
        "prompt_model = PromptedVisionTransformer(prompt_config, vit_config,\n",
        "        img_size=img_size, num_classes=num_classes, vis=False, vit_cfg=vit_config, zero_head=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3PQz0ShGgGB3"
      },
      "outputs": [],
      "source": [
        "#load_pretrain:\n",
        "prompt_model.load_from(np.load(pretrained_dir))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZfEVlJXZhl2H"
      },
      "outputs": [],
      "source": [
        "prompt_model.cuda()\n",
        "epochs = 10\n",
        "lr = 0.01\n",
        "weight_decay=0.0001\n",
        "prompt_model.train()\n",
        "trainable_params = []\n",
        "print_trainable_param = True\n",
        "for p_name, param in prompt_model.named_parameters():\n",
        "  if \"prompt\" not in p_name and \"head\" not in p_name and \"cls_token\" not in p_name:\n",
        "          param.requires_grad = False\n",
        "  else:\n",
        "    if print_trainable_param:\n",
        "          print(\"\\t{}, {}, {}\".format(p_name, param.numel(), param.shape))\n",
        "    # trainable_params.append((key, value))\n",
        "    trainable_params += [{\n",
        "                        \"params\": [param]\n",
        "                    }]\n",
        "prompt_optimizer = torch.optim.SGD(trainable_params, lr=lr, momentum=0.9, weight_decay=weight_decay)\n",
        "# You may consider using a warm_up schedule for this task\n",
        "prompt_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(prompt_optimizer, T_max=epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m6NKIN392DvS"
      },
      "source": [
        "#### Training model fine-tuning with prompts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "7wTYiKAUVNXP"
      },
      "outputs": [],
      "source": [
        "for e in range(epochs):\n",
        "  train_epoch_vit(prompt_model, prompt_optimizer, train_loader, val_loader, prompt_scheduler, e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZRVs0LVVNXP"
      },
      "source": [
        "## Submit to Kaggle for fine-tuning with prompts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OEtZbGW-VNXP"
      },
      "outputs": [],
      "source": [
        "## Create a csv file for fine-tuning with prompts, and submit solution_prompt_finetuning.csv to Kaggle\n",
        "predict_and_save(prompt_model, test_loader, device=\"cuda\", submission_path=\"solution_prompt_finetuning.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zxUIzf8XXb7Y"
      },
      "source": [
        "# III) Model Fine-Tuning with Adapters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lw_7RU8S2aGj"
      },
      "source": [
        "The `ADPT_Block` class is a modified version of a standard Vision Transformer (ViT) block, incorporating additional Adapter layers to enhance model capacity and flexibility. It initializes essential components such as layer normalization for both attention and feed-forward networks, as well as the attention mechanism itself. Depending on the specified adapter configuration style (currently supporting only \"Pfeiffer\"), it sets up downsampling and upsampling linear layers that reduce and restore the hidden size, respectively. In the `forward` method, the block first processes the input through the attention mechanism, followed by the feed-forward network, while integrating Adapter layers between these two processes. The class also includes a `load_from` method that facilitates the loading of weights from a pre-trained model, ensuring that the adapter's parameters are correctly initialized with the corresponding attention and feed-forward weights, as well as the normalization layers' parameters. This structure enables the model to leverage pre-trained knowledge while benefiting from the flexibility of Adapter layers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1kiHBPs32RTi"
      },
      "outputs": [],
      "source": [
        "# Re-define the ViT model with Adapter\n",
        "# The only mayjor difference is ADPT_Block and Block.\n",
        "# ADPT_Block uses additional Adapter blocks\n",
        "\n",
        "class ADPT_Block(nn.Module):\n",
        "    def __init__(self, config, vis, adapter_config):\n",
        "        super(ADPT_Block, self).__init__()\n",
        "        self.hidden_size = config.hidden_size\n",
        "        self.attention_norm = LayerNorm(config.hidden_size, eps=1e-6)\n",
        "        self.ffn_norm = LayerNorm(config.hidden_size, eps=1e-6)\n",
        "        self.ffn = Mlp(config)\n",
        "        self.attn = Attention(config, vis)\n",
        "\n",
        "        self.adapter_config = adapter_config\n",
        "\n",
        "        if adapter_config.STYLE == \"Pfeiffer\":\n",
        "            self.adapter_downsample = nn.Linear(\n",
        "                config.hidden_size,\n",
        "                config.hidden_size // adapter_config.REDUCATION_FACTOR\n",
        "            )\n",
        "            self.adapter_upsample = nn.Linear(\n",
        "                config.hidden_size // adapter_config.REDUCATION_FACTOR,\n",
        "                config.hidden_size\n",
        "            )\n",
        "            self.adapter_act_fn = ACT2FN[\"gelu\"]\n",
        "\n",
        "            nn.init.zeros_(self.adapter_downsample.weight)\n",
        "            nn.init.zeros_(self.adapter_downsample.bias)\n",
        "\n",
        "            nn.init.zeros_(self.adapter_upsample.weight)\n",
        "            nn.init.zeros_(self.adapter_upsample.bias)\n",
        "        else:\n",
        "            raise ValueError(\"Other adapter styles are not supported.\")\n",
        "\n",
        "    def forward(self, x):\n",
        "    # Check if the adapter configuration style is \"Pfeiffer\"\n",
        "      if self.adapter_config.STYLE == \"Pfeiffer\":\n",
        "        # Store the original input x for later residual connection\n",
        "        h = x\n",
        "\n",
        "        # Apply layer normalization to the input x\n",
        "        x = self.attention_norm(x)\n",
        "\n",
        "        # Pass the normalized input through the attention layer\n",
        "        # The output x is the result of the attention mechanism\n",
        "        # 'weights' stores the attention weights for analysis\n",
        "        x, weights = self.attn(x)\n",
        "\n",
        "        # Add the original input (h) to the output of the attention block (residual connection)\n",
        "        x = x + h\n",
        "\n",
        "        # Store the output after attention for another residual connection\n",
        "        h = x\n",
        "\n",
        "        # Apply layer normalization to the output of the attention block\n",
        "        x = self.ffn_norm(x)\n",
        "\n",
        "        # Pass the normalized output through the feed-forward network\n",
        "        x = self.ffn(x)\n",
        "\n",
        "        # Start inserting adapter layers after the feed-forward network\n",
        "        # First, downsample the output using a linear layer\n",
        "        adpt = self.adapter_downsample(x)\n",
        "\n",
        "        # Apply the activation function (e.g., GELU) to the downsampled output\n",
        "        adpt = self.adapter_act_fn(adpt)\n",
        "\n",
        "        # Upsample the activated output back to the original hidden size\n",
        "        adpt = self.adapter_upsample(adpt)\n",
        "\n",
        "        # Add the output of the adapter layers back to the original feed-forward output (x)\n",
        "        x = adpt + x\n",
        "        # End of adapter insertion\n",
        "\n",
        "        # Add the output after the adapter layers to the original input from the attention block (residual connection)\n",
        "        x = x + h\n",
        "\n",
        "        # Return the final output and the attention weights\n",
        "        return x, weights\n",
        "\n",
        "    def load_from(self, weights, n_block):\n",
        "        ROOT = f\"Transformer/encoderblock_{n_block}\"\n",
        "        with torch.no_grad():\n",
        "            query_weight = np2th(weights[pjoin(ROOT, ATTENTION_Q, \"kernel\")]).view(self.hidden_size, self.hidden_size).t()\n",
        "            key_weight = np2th(weights[pjoin(ROOT, ATTENTION_K, \"kernel\")]).view(self.hidden_size, self.hidden_size).t()\n",
        "            value_weight = np2th(weights[pjoin(ROOT, ATTENTION_V, \"kernel\")]).view(self.hidden_size, self.hidden_size).t()\n",
        "            out_weight = np2th(weights[pjoin(ROOT, ATTENTION_OUT, \"kernel\")]).view(self.hidden_size, self.hidden_size).t()\n",
        "\n",
        "            query_bias = np2th(weights[pjoin(ROOT, ATTENTION_Q, \"bias\")]).view(-1)\n",
        "            key_bias = np2th(weights[pjoin(ROOT, ATTENTION_K, \"bias\")]).view(-1)\n",
        "            value_bias = np2th(weights[pjoin(ROOT, ATTENTION_V, \"bias\")]).view(-1)\n",
        "            out_bias = np2th(weights[pjoin(ROOT, ATTENTION_OUT, \"bias\")]).view(-1)\n",
        "\n",
        "            self.attn.query.weight.copy_(query_weight)\n",
        "            self.attn.key.weight.copy_(key_weight)\n",
        "            self.attn.value.weight.copy_(value_weight)\n",
        "            self.attn.out.weight.copy_(out_weight)\n",
        "            self.attn.query.bias.copy_(query_bias)\n",
        "            self.attn.key.bias.copy_(key_bias)\n",
        "            self.attn.value.bias.copy_(value_bias)\n",
        "            self.attn.out.bias.copy_(out_bias)\n",
        "\n",
        "            mlp_weight_0 = np2th(weights[pjoin(ROOT, FC_0, \"kernel\")]).t()\n",
        "            mlp_weight_1 = np2th(weights[pjoin(ROOT, FC_1, \"kernel\")]).t()\n",
        "            mlp_bias_0 = np2th(weights[pjoin(ROOT, FC_0, \"bias\")]).t()\n",
        "            mlp_bias_1 = np2th(weights[pjoin(ROOT, FC_1, \"bias\")]).t()\n",
        "\n",
        "            self.ffn.fc1.weight.copy_(mlp_weight_0)\n",
        "            self.ffn.fc2.weight.copy_(mlp_weight_1)\n",
        "            self.ffn.fc1.bias.copy_(mlp_bias_0)\n",
        "            self.ffn.fc2.bias.copy_(mlp_bias_1)\n",
        "\n",
        "            self.attention_norm.weight.copy_(np2th(weights[pjoin(ROOT, ATTENTION_NORM, \"scale\")]))\n",
        "            self.attention_norm.bias.copy_(np2th(weights[pjoin(ROOT, ATTENTION_NORM, \"bias\")]))\n",
        "            self.ffn_norm.weight.copy_(np2th(weights[pjoin(ROOT, MLP_NORM, \"scale\")]))\n",
        "            self.ffn_norm.bias.copy_(np2th(weights[pjoin(ROOT, MLP_NORM, \"bias\")]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bhi0H_504LqX"
      },
      "source": [
        "The `ADPT_Encoder` class defines a custom encoder module for a vision transformer model that incorporates adapter blocks. In its initialization, it creates a series of adapter layers based on the specified configuration, ensuring that each layer is deep-copied to maintain independent parameters. The `forward` method processes the input hidden states through each adapter block in sequence, collecting the attention weights if visualization is enabled. After passing through all layers, the output hidden states are normalized, and both the normalized states and the attention weights are returned, facilitating subsequent processing or analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fvFG-qFk3YK_"
      },
      "outputs": [],
      "source": [
        "class ADPT_Encoder(nn.Module):\n",
        "    def __init__(self, config, vis, adapter_cfg):\n",
        "        super(ADPT_Encoder, self).__init__()\n",
        "        self.vis = vis\n",
        "        self.layer = nn.ModuleList()\n",
        "        self.encoder_norm = LayerNorm(config.hidden_size, eps=1e-6)\n",
        "\n",
        "        self.num_layers = config.transformer[\"num_layers\"]\n",
        "        for _ in range(self.num_layers):\n",
        "            layer = ADPT_Block(config, vis, adapter_cfg)\n",
        "            self.layer.append(copy.deepcopy(layer))\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        attn_weights = []\n",
        "        for layer_block in self.layer:\n",
        "            hidden_states, weights = layer_block(hidden_states)\n",
        "            if self.vis:\n",
        "                attn_weights.append(weights)\n",
        "        encoded = self.encoder_norm(hidden_states)\n",
        "        return encoded, attn_weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RnsxSM7O4bGP"
      },
      "source": [
        "The `ADPT_Transformer` class defines a transformer model that integrates an embedding layer and an adapter-based encoder. During initialization, it creates an instance of the `Embeddings` class to process input data based on the given configuration and image size, and it initializes the `ADPT_Encoder` to handle the encoded representations. In the `forward` method, the input IDs are transformed into embeddings, which are then passed through the encoder. The method returns both the encoded representations and the attention weights, allowing for further processing or analysis in a vision transformer context."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0cpkEavA4UJ2"
      },
      "outputs": [],
      "source": [
        "class ADPT_Transformer(nn.Module):\n",
        "    def __init__(self, config, img_size, vis, adapter_cfg):\n",
        "        super(ADPT_Transformer, self).__init__()\n",
        "        self.embeddings = Embeddings(config, img_size=img_size)\n",
        "        self.encoder = ADPT_Encoder(config, vis, adapter_cfg)\n",
        "\n",
        "    def forward(self, input_ids):\n",
        "        embedding_output = self.embeddings(input_ids)\n",
        "\n",
        "        encoded, attn_weights = self.encoder(embedding_output)\n",
        "        return encoded, attn_weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMEZ_O6F4pLy"
      },
      "source": [
        "The `ADPT_VisionTransformer` class implements a vision transformer model that utilizes adapter blocks for improved performance and adaptability. Upon initialization, it sets up various components, including a transformer instance (`ADPT_Transformer`) and a linear classification head, depending on the number of classes specified. The `forward` method processes input images, passing them through the transformer and obtaining logits from the first token's output. If the `vis` flag is set, it also returns attention weights. Additionally, the `load_from` method facilitates loading pre-trained weights into the model's layers, managing various scenarios such as zeroing the head or resizing position embeddings. This structure allows the model to adapt efficiently while benefiting from the capabilities of the transformer architecture."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z3u5vi3HXfUC"
      },
      "outputs": [],
      "source": [
        "class ADPT_VisionTransformer(nn.Module):\n",
        "    def __init__(\n",
        "        self, model_type=None,\n",
        "        img_size=224, num_classes=21843, vis=False, adapter_cfg=None, vit_cfg=None, zero_head=False\n",
        "    ):\n",
        "        super(ADPT_VisionTransformer, self).__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.zero_head = zero_head\n",
        "        self.classifier = config.classifier\n",
        "\n",
        "        self.transformer = ADPT_Transformer(vit_cfg, img_size, vis, adapter_cfg)\n",
        "        self.head = Linear(config.hidden_size, num_classes) if num_classes > 0 else nn.Identity()\n",
        "\n",
        "    def forward(self, x, label=None, vis=False):\n",
        "        x, attn_weights = self.transformer(x)\n",
        "        logits = self.head(x[:, 0])\n",
        "\n",
        "        if not vis:\n",
        "            return logits\n",
        "        return logits, attn_weights\n",
        "\n",
        "    def load_from(self, weights):\n",
        "        with torch.no_grad():\n",
        "            if self.zero_head:\n",
        "                nn.init.zeros_(self.head.weight)\n",
        "                nn.init.zeros_(self.head.bias)\n",
        "            else:\n",
        "                self.head.weight.copy_(np2th(weights[\"head/kernel\"]).t())\n",
        "                self.head.bias.copy_(np2th(weights[\"head/bias\"]).t())\n",
        "\n",
        "            self.transformer.embeddings.patch_embeddings.weight.copy_(np2th(weights[\"embedding/kernel\"], conv=True))\n",
        "            self.transformer.embeddings.patch_embeddings.bias.copy_(np2th(weights[\"embedding/bias\"]))\n",
        "            self.transformer.embeddings.cls_token.copy_(np2th(weights[\"cls\"]))\n",
        "            self.transformer.encoder.encoder_norm.weight.copy_(np2th(weights[\"Transformer/encoder_norm/scale\"]))\n",
        "            self.transformer.encoder.encoder_norm.bias.copy_(np2th(weights[\"Transformer/encoder_norm/bias\"]))\n",
        "\n",
        "            posemb = np2th(weights[\"Transformer/posembed_input/pos_embedding\"])\n",
        "            posemb_new = self.transformer.embeddings.position_embeddings\n",
        "            if posemb.size() == posemb_new.size():\n",
        "                self.transformer.embeddings.position_embeddings.copy_(posemb)\n",
        "            else:\n",
        "                print(\"load_pretrained: resized variant: %s to %s\" % (posemb.size(), posemb_new.size()))\n",
        "                ntok_new = posemb_new.size(1)\n",
        "\n",
        "                if self.classifier == \"token\":\n",
        "                    posemb_tok, posemb_grid = posemb[:, :1], posemb[0, 1:]\n",
        "                    ntok_new -= 1\n",
        "                else:\n",
        "                    posemb_tok, posemb_grid = posemb[:, :0], posemb[0]\n",
        "\n",
        "                gs_old = int(np.sqrt(len(posemb_grid)))\n",
        "                gs_new = int(np.sqrt(ntok_new))\n",
        "                print('load_pretrained: grid-size from %s to %s' % (gs_old, gs_new))\n",
        "                posemb_grid = posemb_grid.reshape(gs_old, gs_old, -1)\n",
        "\n",
        "                zoom = (gs_new / gs_old, gs_new / gs_old, 1)\n",
        "                posemb_grid = ndimage.zoom(posemb_grid, zoom, order=1)\n",
        "                posemb_grid = posemb_grid.reshape(1, gs_new * gs_new, -1)\n",
        "                posemb = np.concatenate([posemb_tok, posemb_grid], axis=1)\n",
        "                self.transformer.embeddings.position_embeddings.copy_(np2th(posemb))\n",
        "\n",
        "            for bname, block in self.transformer.encoder.named_children():\n",
        "                for uname, unit in block.named_children():\n",
        "                    unit.load_from(weights, n_block=uname)\n",
        "\n",
        "            if self.transformer.embeddings.hybrid:\n",
        "                self.transformer.embeddings.hybrid_model.root.conv.weight.copy_(np2th(weights[\"conv_root/kernel\"], conv=True))\n",
        "                gn_weight = np2th(weights[\"gn_root/scale\"]).view(-1)\n",
        "                gn_bias = np2th(weights[\"gn_root/bias\"]).view(-1)\n",
        "                self.transformer.embeddings.hybrid_model.root.gn.weight.copy_(gn_weight)\n",
        "                self.transformer.embeddings.hybrid_model.root.gn.bias.copy_(gn_bias)\n",
        "\n",
        "                for bname, block in self.transformer.embeddings.hybrid_model.body.named_children():\n",
        "                    for uname, unit in block.named_children():\n",
        "                        unit.load_from(weights, n_block=bname, n_unit=uname)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KQMUM2T3caoZ"
      },
      "outputs": [],
      "source": [
        "def get_adapter_config():\n",
        "  config = ml_collections.ConfigDict()\n",
        "  config.REDUCATION_FACTOR = 8\n",
        "  config.STYLE = \"Pfeiffer\"\n",
        "  return config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GoC3hP7IYLpS"
      },
      "outputs": [],
      "source": [
        "adapter_config = get_adapter_config()\n",
        "vit_config = get_b16_config()\n",
        "\n",
        "adapter_model = ADPT_VisionTransformer(adapter_cfg=adapter_config, vit_cfg=vit_config, img_size=img_size, zero_head=True, num_classes=num_classes)\n",
        "adapter_model.load_from(np.load(pretrained_dir))\n",
        "adapter_model = adapter_model.cuda()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "Hyperparameters for finetuning with adapters\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "cWnxKFSQa9VS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NKLqhQ2oaMIv"
      },
      "outputs": [],
      "source": [
        "epochs = 10\n",
        "lr = 0.01\n",
        "weight_decay=0.0001\n",
        "adapter_model.train()\n",
        "adapter_trainable_params = []\n",
        "print(\"Trainable params:\")\n",
        "for p_name, param in adapter_model.named_parameters():\n",
        "  if \"adapter\" not in p_name and \"head\" not in p_name and \"cls_token\" not in p_name:\n",
        "          param.requires_grad = False\n",
        "  else:\n",
        "    print(\"\\t{}, {}, {}\".format(p_name, param.numel(), param.shape))\n",
        "    # trainable_params.append((key, value))\n",
        "    adapter_trainable_params += [{\n",
        "                        \"params\": [param]\n",
        "                    }]\n",
        "adapter_optimizer = torch.optim.SGD(adapter_trainable_params, lr=lr, momentum=0.9, weight_decay=weight_decay)\n",
        "# You may consider using a warm_up schedule for this task\n",
        "adapter_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(adapter_optimizer, T_max=epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NZ72Ug5r5D34"
      },
      "source": [
        "#### Training model fine-tuning with adapters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MbQjUpvsazkk",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "for e in range(epochs):\n",
        "  train_epoch_vit(adapter_model, adapter_optimizer, train_loader, test_loader, adapter_scheduler, e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L_U8tNFMVNXR"
      },
      "source": [
        "## Submit to Kaggle for fine-tuning with Adapters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VGJlMxFYVNXR"
      },
      "outputs": [],
      "source": [
        "## Create a csv file for fine-tuning with adapters and submit solution_adapters_finetuning.csv to Kaggle\n",
        "predict_and_save(adapter_model, test_loader, device=\"cuda\", submission_path=\"solution_adapters_finetuning.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s20XeUsuVNXS"
      },
      "source": [
        "# Notes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQ-CDZS1VNXS"
      },
      "source": [
        "You need to download the `solution_normal_vit_finetuning.csv`, `solution_prompt_finetuning.csv`, or  `solution_adapters_finetuning.csv` files and upload them to the provided Kaggle competition URL\n",
        "\n",
        "Remember to change your display name on the Leaderboard to:\n",
        " \\<**Your team name**\\>\n",
        "\n",
        "\n",
        "\n",
        "Tutorial 4: https://www.kaggle.com/t/5eed50209f4b4b40a23a8343117eebdd\n",
        "\n",
        "Tutorial 3: https://www.kaggle.com/t/115e016843df4f0f8060701a39a8b339"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}